{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 67818 Applied Competitive Lab in Data Science\n",
    "\n",
    "## Final Project\n",
    "\n",
    "### Participants:\n",
    "\n",
    "- **Name:** Dan Badur\n",
    "- **Student ID:** 209019256\n",
    "- \n",
    "- **Name:** Tzur Breen\n",
    "- **Student ID:** 209354919\n",
    "- \n",
    "- **Name:** Shir Elbilia\n",
    "- **Student ID:** 208621102\n",
    "- \n",
    "- **Name:** Eliya Hasson\n",
    "- **Student ID:** 208845032\n",
    "- \n",
    "- **Name:** Tal Barda\n",
    "- **Student ID:** 208729210"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Preparation and Feature Engineering:\n",
    "\n",
    "We will start by defining a function for calculating the distance between each point in our database and a set of polygons in a wanted shapefile.\n",
    "The function is set to run on multiple cors for better performance. \n",
    "\n",
    "For our original data containing 571000 rows for an 8 core CPU (the code utilizes 0.75 of the cores):\n",
    "Calculating for the nature polygons takes about 3 minutes.\n",
    "For the others it's about 30 seconds to 1 minute each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T09:24:43.252556Z",
     "start_time": "2024-02-26T09:17:48.676586Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8120\\555970788.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "import math\n",
    "from distance_calculations import calculate_distances_chunk\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "data_df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "total_cores = os.cpu_count()\n",
    "cores_to_use = math.ceil(total_cores * 0.75)\n",
    "num_cores = max(1, cores_to_use) \n",
    "WGS_84_GEO = \"EPSG:4326\"\n",
    "\n",
    "def calculate_distances_gdf_to_polygon_parallel(points_gdf, polygons_gdf, target_name, attribute_name=None):\n",
    "\n",
    "    num_chunks = num_cores * 4  \n",
    "    points_chunks = np.array_split(points_gdf, num_chunks)  \n",
    "    \n",
    "    with tqdm(total=len(points_chunks), desc=\"Processing chunks for \"+target_name) as pbar:\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=num_cores) as executor:\n",
    "            futures = [executor.submit(calculate_distances_chunk, chunk.geometry, polygons_gdf, return_attribute=attribute_name) for chunk in points_chunks]\n",
    "            \n",
    "            results = []\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                results.extend(future.result())\n",
    "                pbar.update(1)\n",
    "    \n",
    "    return results\n",
    "\n",
    "data_gdf = gpd.GeoDataFrame(data_df, geometry=gpd.points_from_xy(data_df.LONGITUDE, data_df.LATITUDE), crs=WGS_84_GEO)\n",
    "\n",
    "nature_gdf = gpd.read_file('Nature/Nature.shp')\n",
    "powerline_gdf = gpd.read_file('Powerline/Powerline.shp')\n",
    "camp_gdf = gpd.read_file('Campgrounds/Campgrounds.shp')\n",
    "population_gdf = gpd.read_file('Population/Population.shp')\n",
    "railroads_gdf = gpd.read_file('Railroads/Railroads.shp')\n",
    "schools_gdf = gpd.read_file('PublicSchools/PublicSchools.shp')\n",
    "#city_gdf =  gpd.read_file('City/City.shp')\n",
    "\n",
    "nature_gdf = nature_gdf.to_crs(WGS_84_GEO)\n",
    "powerline_gdf = powerline_gdf.to_crs(WGS_84_GEO)\n",
    "camp_gdf = camp_gdf.to_crs(WGS_84_GEO)\n",
    "population_gdf = population_gdf.to_crs(WGS_84_GEO)\n",
    "railroads_gdf = railroads_gdf.to_crs(WGS_84_GEO)\n",
    "schools_gdf = schools_gdf.to_crs(WGS_84_GEO)\n",
    "#city_gdf = city_gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "data_df['distance_to_nearest_nature_km'] = calculate_distances_gdf_to_polygon_parallel(data_gdf, nature_gdf, \"Nature\")\n",
    "data_df['distance_to_nearest_powerline_km'] = calculate_distances_gdf_to_polygon_parallel(data_gdf, powerline_gdf, \"Powerline\")\n",
    "data_df['distance_to_nearest_campground_km'] = calculate_distances_gdf_to_polygon_parallel(data_gdf, camp_gdf, \"Campground\")\n",
    "data_df['distance_to_nearest_railroad_km'] = calculate_distances_gdf_to_polygon_parallel(data_gdf, railroads_gdf, \"Railroad\")\n",
    "data_df['distance_to_nearest_school_km'] = calculate_distances_gdf_to_polygon_parallel(data_gdf, schools_gdf, \"School\")\n",
    "\n",
    "distance_from_population_and_class = calculate_distances_gdf_to_polygon_parallel(data_gdf, population_gdf, \"Population\", \"POP_CLASS\")\n",
    "data_df['distance_to_nearest_population'], data_df['nearest_population_class'] = zip(*distance_from_population_and_class)\n",
    "\n",
    "# data_df['distance_to_nearest_city_km'] = calculate_distances_gdf_to_polygon_parallel(data_gdf, city_gdf, num_cores, \"City\")\n",
    "\n",
    "data_df.to_csv('data_with_distances.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate the averages for each cause of fire for every feature we calculated above: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T09:25:04.760192Z",
     "start_time": "2024-02-26T09:24:56.710204Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"data_with_distances.csv\")\n",
    "\n",
    "# Get unique values in the STAT_CAUSE_DESCR column\n",
    "unique_causes = df['STAT_CAUSE_DESCR'].unique()\n",
    "\n",
    "for cause in unique_causes:\n",
    "    # Filter the dataframe for the current cause\n",
    "    cause_df = df[df['STAT_CAUSE_DESCR'] == cause]\n",
    "\n",
    "    # Calculate the average distances\n",
    "    avg_nature_distance = cause_df['distance_to_nearest_nature_km'].mean()\n",
    "    avg_powerline_distance = cause_df['distance_to_nearest_powerline_km'].mean()\n",
    "    avg_camp_distance = cause_df['distance_to_nearest_campground_km'].mean()\n",
    "    avg_population_distance = cause_df['distance_to_nearest_population'].mean()\n",
    "    avg_city_population_class = cause_df['nearest_population_class'].mean()\n",
    "    avg_city_distance_railroad= cause_df['distance_to_nearest_railroad_km'].mean()\n",
    "    avg_school_distance = cause_df['distance_to_nearest_school_km'].mean()\n",
    "\n",
    "\n",
    "    # Print the averages for the current cause\n",
    "    print(f\"Averages {cause}:\")\n",
    "    print(f\"  Nature distance: {avg_nature_distance:.2f} km\")\n",
    "    print(f\"  Campground distance: {avg_camp_distance:.2f} km\")\n",
    "    print(f\"  Nearest population distance: {avg_population_distance:.2f} km\")\n",
    "    print(f\"  Nearest population class: {avg_city_population_class:.2f}\")\n",
    "    print(f\"  Nearest railroad distance: {avg_city_distance_railroad:.2f} km\")\n",
    "    print(f\"  Powerline distance: {avg_powerline_distance:.2f} km\")\n",
    "    print(f\"  School distance: {avg_school_distance:.2f} km\")\n",
    "\n",
    "    # print(f\"  City: {avg_city_distance:.2f} km\")\n",
    "    print()\n",
    "\n",
    "print()\n",
    "df = pd.read_csv(\"data_with_distances.csv\")\n",
    "avg_city_distance_by_fire_size = df.groupby('FIRE_SIZE_CLASS')['distance_to_nearest_population'].mean()\n",
    "\n",
    "print(\"Average distance from the nearest population for each FIRE_SIZE_CLASS:\")\n",
    "print(avg_city_distance_by_fire_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will explore time related features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We worked on adding and exploring features related to time.\n",
    "We decided to check whether there is a connection between holidays and the type of the cause of the fire.\n",
    "The rational was that during holidays and vacations, people are returning home and may have more time to travel (campfire) or they just get bored and do some stupid things (children, arson...).\n",
    "Therefore, we downloaded a dataset that contains the dates of some holidays in the USA and added to our dataset the distance of every fire from the closest holiday in days.\n",
    "The added features are \"nearest_holiday\", \"days_from_closest_holiday\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_dates_df = pd.read_csv(f\"US Holiday Dates (2004-2021).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Date' in holiday_dates_df to datetime objects\n",
    "holiday_dates_df['Date'] = pd.to_datetime(holiday_dates_df['Date'])\n",
    "\n",
    "# Create a dictionary to map each year to its holidays\n",
    "holiday_dict = {}\n",
    "for year in holiday_dates_df['Year'].unique():\n",
    "    holiday_dict[year] = holiday_dates_df[holiday_dates_df['Year'] == year]\n",
    "\n",
    "def find_nearest_holiday(fire_year, discovery_doy):\n",
    "    if fire_year not in holiday_dict:\n",
    "        return (\"No Data\", \"No Data\")\n",
    "    year_holidays = holiday_dict[fire_year]\n",
    "    fire_date = datetime(fire_year, 1, 1) + timedelta(days=discovery_doy - 1)\n",
    "    nearest_holiday = None\n",
    "    min_days_diff = float('inf')\n",
    "    for _, row in year_holidays.iterrows():\n",
    "        holiday_date = row['Date']\n",
    "        diff = abs((fire_date - holiday_date).days)\n",
    "        if diff < min_days_diff:\n",
    "            min_days_diff = diff\n",
    "            nearest_holiday = row['Holiday']\n",
    "    return (nearest_holiday, min_days_diff)\n",
    "\n",
    "# Apply the function to each row in the wildfire dataset\n",
    "data_df['nearest_holiday'], data_df['days_from_closest_holiday'] = zip(*data_df.apply(lambda row: find_nearest_holiday(row['FIRE_YEAR'], row['DISCOVERY_DOY']), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also noticed that there are specific reasons that have more incidents along different hours during the day.\n",
    "We considered adding a feature of \"part_of_day\" in which the fire took place, but we realized that we already have the distribution by hours so this feature will not be very useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also add here a season feature that will help predict the cause type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['DISCOVERY_DATE'] = pd.to_datetime(data_df['DISCOVERY_DATE'], unit='D', origin='julian')\n",
    "def get_season(date):\n",
    "    month = date.month\n",
    "    if month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'Fall'\n",
    "    else:\n",
    "        return 'Winter'\n",
    "data_df['Season'] = data_df['DISCOVERY_DATE'].apply(get_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
